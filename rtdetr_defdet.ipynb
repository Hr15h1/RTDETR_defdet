{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08194947",
   "metadata": {},
   "source": [
    "Defect Detection using Real Time Detection Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97350558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in defects_dataset_new-1 to coco:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 715421/715421 [00:54<00:00, 13068.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to defects_dataset_new-1 in coco:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12413/12413 [00:05<00:00, 2408.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"UjVLCDuAMxom6CBhWdDX\")\n",
    "project = rf.workspace(\"defect-detection-guglj\").project(\"defects_dataset_new-utjup\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed092bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import RTDETR\n",
    "import torch\n",
    "\n",
    "model = RTDETR(\"test.yaml\")\n",
    "dummy = torch.rand(1, 3, 640, 640)\n",
    "with torch.no_grad():\n",
    "    out = model(dummy)\n",
    "    print(out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395000a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import RTDETR\n",
    "import torch\n",
    "\n",
    "# Load your RT-DETR DenseNet model\n",
    "model = RTDETR(\"test.yaml\")\n",
    "\n",
    "# The first module is the backbone\n",
    "backbone = model.model.model[0]\n",
    "\n",
    "# Dummy input\n",
    "x = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "# Run through the backbone to see feature maps\n",
    "with torch.no_grad():\n",
    "    features = backbone(x)\n",
    "    print(\"\\n--- Backbone outputs ---\")\n",
    "    for i, f in enumerate(features):\n",
    "        print(f\"Index {i}: {tuple(f.shape)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f6385f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base RT-DETR model...\n",
      "WARNING no model scale passed. Assuming scale='l'.\n",
      "Replacing backbone with SqueezeNet...\n",
      "Loaded pretrained weights for squeezenet1_1\n",
      "Adding AIFI neck...\n",
      "Configuring RTDETRDecoder head...\n",
      "âœ… Custom RTDETR model initialized successfully.\n",
      "Model parameters: 10302534\n",
      "\n",
      "âœ… Forward succeeded.\n",
      "Output is tuple of length: 2\n",
      "  [0] Tensor shape: (1, 300, 10)\n",
      "  [1] nested len: 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from rtdetr_custom import RTDETRCustom\n",
    "\n",
    "# --- Instantiate and test ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = RTDETRCustom().to(device)\n",
    "model.eval()\n",
    "\n",
    "dummy = torch.randn(1, 3, 640, 640).to(device)\n",
    "with torch.no_grad():\n",
    "    out = model(dummy)\n",
    "\n",
    "print(\"\\nâœ… Forward succeeded.\")\n",
    "if isinstance(out, tuple):\n",
    "    print(\"Output is tuple of length:\", len(out))\n",
    "    for i, o in enumerate(out):\n",
    "        if torch.is_tensor(o):\n",
    "            print(f\"  [{i}] Tensor shape:\", tuple(o.shape))\n",
    "        elif isinstance(o, (list, tuple)):\n",
    "            print(f\"  [{i}] nested len:\", len(o))\n",
    "else:\n",
    "    print(\"Output type:\", type(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a170a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import densenet121\n",
    "\n",
    "m = densenet121(weights=\"DEFAULT\").features.eval()\n",
    "x = torch.randn(1, 3, 640, 640)\n",
    "out = []\n",
    "for i, (name, layer) in enumerate(m._modules.items()):\n",
    "    x = layer(x)\n",
    "    if name in [\"denseblock1\", \"denseblock2\", \"denseblock3\", \"denseblock4\"]:\n",
    "        out.append((name, x.shape))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bd08d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "\n",
    "# Load pretrained EfficientNet-V2-M backbone\n",
    "m = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.DEFAULT)\n",
    "m.eval()\n",
    "\n",
    "# Create a feature extractor that outputs intermediate feature maps\n",
    "# This is available in torchvision >= 0.13\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "# Specify which layers you want to extract\n",
    "return_nodes = {\n",
    "    'features.3': 'stage1',   # stride 8   -> (80, 80)\n",
    "    'features.6': 'stage2',   # stride 16  -> (160, 40)\n",
    "    'features.8': 'stage3',   # stride 32  -> (1280, 20)\n",
    "}\n",
    "\n",
    "feature_extractor = create_feature_extractor(m, return_nodes=return_nodes)\n",
    "\n",
    "# Test with dummy input\n",
    "x = torch.rand(1, 3, 640, 640)\n",
    "with torch.no_grad():\n",
    "    out = feature_extractor(x)\n",
    "\n",
    "# Print each output shape\n",
    "for k, v in out.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968ab2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "\n",
    "# Load pretrained EfficientNetV2-M backbone\n",
    "model = models.efficientnet_b0(weights=\"DEFAULT\")\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Dummy input tensor\n",
    "x = torch.randn(1, 3, 640, 640)\n",
    "\n",
    "# Dictionary to store shapes\n",
    "shapes = {}\n",
    "\n",
    "# Hook function to record tensor shapes\n",
    "def hook_fn(name):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, torch.Tensor):\n",
    "            shapes[name] = tuple(output.shape)\n",
    "        elif isinstance(output, (list, tuple)):\n",
    "            shapes[name] = [tuple(o.shape) for o in output if isinstance(o, torch.Tensor)]\n",
    "    return hook\n",
    "\n",
    "# Register forward hooks for all layers\n",
    "hooks = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Conv2d, nn.BatchNorm2d, nn.ReLU, nn.SiLU, nn.Sequential, nn.AdaptiveAvgPool2d)):\n",
    "        hooks.append(module.register_forward_hook(hook_fn(name)))\n",
    "\n",
    "# Run forward pass\n",
    "with torch.no_grad():\n",
    "    _ = model(x)\n",
    "\n",
    "# Print layer names and output shapes\n",
    "print(\"\\nðŸ“Š EfficientNetV2-M Layer Output Shapes:\\n\")\n",
    "for name, shape in shapes.items():\n",
    "    print(f\"{name:<40} -> {shape}\")\n",
    "\n",
    "# Remove hooks\n",
    "for h in hooks:\n",
    "    h.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b4bfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 300 2s, 58.8ms\n",
      "Speed: 0.7ms preprocess, 58.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5'}\n",
      "obb: None\n",
      "orig_img: array([[[184, 123,  30],\n",
      "        [ 55,   6, 174],\n",
      "        [ 96, 133, 192],\n",
      "        ...,\n",
      "        [ 57,  63,  94],\n",
      "        [238,  72, 114],\n",
      "        [145, 145, 178]],\n",
      "\n",
      "       [[128, 115,  39],\n",
      "        [168, 171, 241],\n",
      "        [125,  92, 254],\n",
      "        ...,\n",
      "        [227, 191, 123],\n",
      "        [205, 109,  68],\n",
      "        [226, 225,  14]],\n",
      "\n",
      "       [[ 80, 138, 126],\n",
      "        [112,  60,  36],\n",
      "        [245, 184, 225],\n",
      "        ...,\n",
      "        [186, 186,  16],\n",
      "        [ 59,  52,  85],\n",
      "        [ 81, 173, 188]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[180,  17,  43],\n",
      "        [ 46, 130, 246],\n",
      "        [176, 147,  78],\n",
      "        ...,\n",
      "        [  9,  55, 175],\n",
      "        [115, 169, 204],\n",
      "        [ 73,  78, 224]],\n",
      "\n",
      "       [[134, 202,  99],\n",
      "        [149,  81,   0],\n",
      "        [ 91,  35, 177],\n",
      "        ...,\n",
      "        [122, 249,  94],\n",
      "        [ 30, 120, 140],\n",
      "        [ 23, 202,  74]],\n",
      "\n",
      "       [[114, 129, 211],\n",
      "        [191, 154, 211],\n",
      "        [219, 244,  11],\n",
      "        ...,\n",
      "        [192, 117, 176],\n",
      "        [232, 214, 220],\n",
      "        [222, 197, 163]]], shape=(640, 640, 3), dtype=uint8)\n",
      "orig_shape: (640, 640)\n",
      "path: 'image0.jpg'\n",
      "probs: None\n",
      "save_dir: 'C:\\\\Users\\\\USER X\\\\Desktop\\\\rtdetr_defdet\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 0.667500076815486, 'inference': 58.75650001689792, 'postprocess': 2.303300076164305}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import RTDETR\n",
    "\n",
    "model = RTDETR(\"rtdetr_densenet121.yaml\")\n",
    "\n",
    "dummy = torch.rand(1, 3, 640, 640)\n",
    "with torch.no_grad():\n",
    "    out = model(dummy)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RTDETR(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "defdet (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
